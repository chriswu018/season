<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment">
  <meta name="keywords" content="Hallucination, Multimodal LLMs, Video Understanding, Contrastive Learning, SANTA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SANTA: Mitigating Object and Action Hallucinations in Multimodal LLMs</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- MathJax -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://github.com/kaipochang">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://arxiv.org/abs/2402.11411">
              VCD (CVPR 2024)
            </a>
            <a class="navbar-item" href="https://arxiv.org/abs/2405.14815">
              Vista-LLaMA
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Mitigating Object and Action Hallucinations in Multimodal
              LLMs<br>via Self-Augmented Contrastive Alignment</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#">Anonymous Authors</a><sup>1</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>WACV 2026 Submission #290</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://drive.google.com/file/d/130I3DZtX5LPSsPdQVLQpvBmxyLJo_4Ny/view?usp=sharing"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser Image Section -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="publication-banner">
          <img src="./figs/teaser.png" alt="Teaser Image" style="width: 100%; height: auto;">
        </div>
        <h2 class="subtitle has-text-centered">
          Compared to existing MLLMs suffering from object and action hallucinations, our SANTA enhances faithfulness in
          describing both visual objects and temporal actions.
        </h2>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate
              descriptive captions for input videos. However, these models suffer from factual inaccuracies in the
              generated descriptions, causing severe hallucination issues. While prior works have explored alleviating
              hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for
              dynamic videos remains a challenging and unsolved task.
            </p>
            <p>
              To tackle this challenge, we propose a <strong>Self-Augmented Contrastive Alignment (SANTA)</strong>
              framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing
              the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the
              potential hallucinations that lie in the MLLM and transform the original captions to the contrasted
              negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects
              and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments
              demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations,
              yielding superior performance on the hallucination examination benchmarks.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Task. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Task Definition</h2>
          <div class="content has-text-justified">
            <p>
              <strong>Input:</strong> A dynamic video sequence $v$.<br>
              <strong>Output:</strong> A faithful textual caption $c$ that accurately describes the visual objects and
              temporal actions in the video, without hallucinating non-existent elements.
            </p>
            <div class="box has-background-light">
              <h4 class="title is-5">Example Scenario</h4>
              <p>
                Consider a video showing a person cleaning a floor.
              <ul>
                <li><strong>Faithful Description:</strong> "A person is <em>mopping</em> the floor with a <em>mop</em>."
                </li>
                <li><strong>Hallucinated Description (Object):</strong> "A person is mopping the floor with a <span
                    style="color: red;">broom</span>." (Object Hallucination)</li>
                <li><strong>Hallucinated Description (Action):</strong> "A person is <span
                    style="color: red;">sweeping</span> the floor." (Action Hallucination)</li>
              </ul>
              The goal is to ensure the model generates the faithful description by correctly identifying both the
              object ("mop") and the action ("mopping") from the video dynamics.
              </p>
            </div>
          </div>
        </div>
      </div>
      <!--/ Task. -->

      <!-- Challenges. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Challenges & Motivation</h2>
          <div class="content has-text-justified">
            <p>
              Mitigating hallucinations in video MLLMs presents unique challenges compared to static images:
            </p>
            <ul>
              <li><strong>Dynamic Complexity:</strong> Videos contain temporal dynamics that are harder to capture than
                static visual features. Models often fail to distinguish similar actions (e.g., "picking up" vs.
                "putting down") or track objects across frames.</li>
              <li><strong>Language Priors:</strong> MLLMs often rely on strong language priors (spurious correlations)
                rather than visual evidence. For instance, if "person" and "kitchen" often appear with "cooking" in
                training text, the model might hallucinate "cooking" even if the person is just standing there.</li>
              <li><strong>Limitations of Existing Works:</strong>
                <ul>
                  <li><em>Computationally Expensive:</em> Methods like VCD or DeCo require extra decoding steps, slowing
                    down inference.</li>
                  <li><em>Image-Focused:</em> Methods like HACL or HALVA focus on static object hallucinations and fail
                    to address temporal action hallucinations crucial for video understanding.</li>
                </ul>
              </li>
            </ul>
          </div>
        </div>
      </div>
      <!--/ Challenges. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Key Idea. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Key Idea: SANTA Framework</h2>
          <div class="content has-text-justified">
            <p>
              The core idea of <strong>SANTA (Self-Augmented Contrastive Alignment)</strong> is to leverage the model's
              own tendency to hallucinate to improve its faithfulness.
            </p>
            <blockquote class="is-size-5">
              "By identifying what the model is likely to hallucinate and using those hallucinations as negative
              examples during training, we can teach the model to distinguish between factual visual evidence and
              spurious language correlations."
            </blockquote>
            <p>
              SANTA achieves this through two intuitive steps:
            <ol>
              <li><strong>Self-Generated Hallucinations:</strong> We ask the model to generate captions that it thinks
                are likely but are actually incorrect (hallucinations). These serve as "hard negatives" â€” examples of
                what <em>not</em> to generate.</li>
              <li><strong>Fine-Grained Alignment:</strong> We align specific visual regions (objects) and temporal
                dynamics (actions) with their corresponding words in the text. This ensures the model isn't just looking
                at the whole video vaguely, but is paying attention to the specific "mop" when it says "mop", and the
                specific "mopping" motion when it says "mopping".</li>
            </ol>
            </p>
          </div>
        </div>
      </div>
      <!--/ Key Idea. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Method. -->
      <h2 class="title is-3 has-text-centered">Methodology</h2>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h3 class="title is-4">1. Overview</h3>
          <div class="content has-text-justified">
            <p>
              The SANTA framework operates during the training phase of the MLLM. It introduces auxiliary contrastive
              loss functions that run alongside the standard next-token prediction loss. The goal is to pull the
              representations of visual facts (objects/actions) closer to their correct textual descriptions and push
              them away from hallucinated descriptions.
            </p>
            <img src="./figs/overview.png" alt="SANTA Framework Overview"
              style="width:100%; margin-bottom: 20px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h3 class="title is-4">2. Training: Self-Augmented Contrastive Alignment</h3>
          <div class="content has-text-justified">
            <p>
              The training process consists of three alignment objectives:
            </p>

            <h5 class="title is-6">A. Hallucinative Self-Augmentation (Generating Negatives)</h5>
            <p>
              Instead of using generic negative samples, we generate <em>hallucinative captions</em>. We ask the
              frozen MLLM to predict the next token but force it to choose the highest-probability token that is
              <strong>NOT</strong> in the ground truth. This creates a caption that looks plausible to the language
              model but is factually wrong. These become our hard negatives.
            </p>

            <h5 class="title is-6">B. Object-Level Alignment</h5>
            <p>
              We extract visual object tracklets using Grounding-SAM2. We align these visual features with the
              corresponding object phrases in the text.
              The negatives include other objects in the batch AND the objects from our generated hallucinated captions.
            </p>

            <h5 class="title is-6">C. Action-Level Alignment</h5>
            <p>
              Actions are more abstract. We use a <strong>Perceiver-based Action Squeezer</strong> that takes object
              tracklets and learns to extract "action features" by looking at how objects interact over time.
              This ensures the model pays attention to the <em>dynamics</em> between objects, not just their static
              appearance.
            </p>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h3 class="title is-4">3. Inference</h3>
          <div class="content has-text-justified">
            <p>
              During inference, we simply use the trained MLLM to generate captions. Because the model was trained with
              SANTA, its internal representations are now more robust against language priors and more grounded in
              visual evidence. No extra decoding steps or external models are needed at inference time, keeping it fast.
            </p>
          </div>
        </div>
      </div>
      <!--/ Method. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Data & Benchmarks. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Data and Evaluation</h2>
          <div class="content has-text-justified">
            <p>
              <strong>Training Data:</strong> We use <strong>MiraData</strong>, a large-scale video dataset with
              high-quality, structured captions (42k videos, avg 72s duration).
            </p>
            <p>
              <strong>Evaluation Benchmarks:</strong>
            <ul>
              <li><strong>MiraData-9k:</strong> For evaluating captioning quality and hallucination.</li>
              <li><strong>FactVC:</strong> A benchmark specifically designed for factual video captioning.</li>
              <li><strong>VidHal:</strong> A multiple-choice QA benchmark for hallucination.</li>
            </ul>
            </p>
            <p>
              <strong>Metrics:</strong> We use standard metrics like FactVC-F1 and a proposed
              <strong>Weighted-HalFscore</strong> (an improvement over HalFscore that handles synonyms and token
              importance better).
            </p>
          </div>
        </div>
      </div>
      <!--/ Data & Benchmarks. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Results. -->
      <h2 class="title is-3 has-text-centered">Experimental Results</h2>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h4 class="title is-4">1. Video Captioning Performance (MiraData-9k)</h4>
          <div class="content has-text-centered">
            <p>SANTA significantly outperforms baselines in reducing both object and action hallucinations.</p>
            <img src="./tabs/miradata.png" alt="MiraData Results"
              style="max-width: 100%; height: auto; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
          </div>

          <h4 class="title is-4">2. Factual Consistency (FactVC)</h4>
          <div class="content has-text-centered">
            <img src="./tabs/factvc.png" alt="FactVC Results"
              style="max-width: 100%; height: auto; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
          </div>

          <h4 class="title is-4">3. Hallucination QA (VidHal)</h4>
          <div class="content has-text-centered">
            <img src="./tabs/vidhal.png" alt="VidHal Results"
              style="max-width: 100%; height: auto; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
          </div>
        </div>
      </div>
      <!--/ Results. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Analysis. -->
      <h2 class="title is-3 has-text-centered">Analysis & Qualitative Results</h2>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h3 class="title is-4">Qualitative Comparison</h3>
          <div class="content has-text-justified">
            <p>
              Visualizing the difference between baseline and SANTA. In the example below, the baseline hallucinates a
              "broom" and "sweeping", while SANTA correctly identifies the "mop" and "mopping".
            </p>
            <!-- Placeholder for qualitative image -->
            <div class="has-text-centered">
              <!-- Using PNG for high quality qualitative results -->
              <img src="./figs/main_qualitative.png" alt="Qualitative Results"
                style="width: 100%; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
              <p class="is-size-7">Figure 4 from paper: Qualitative comparison on FactVC.</p>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h3 class="title is-4">Feature Space Analysis (t-SNE)</h3>
          <div class="content has-text-justified">
            <p>
              We visualized the feature space of the MLLM before and after SANTA training.
            </p>
            <div class="has-text-centered">
              <img src="./figs/tsne.png" alt="t-SNE Analysis"
                style="width: 100%; max-width: 800px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
            </div>
            <p>
              <strong>Before SANTA:</strong> There is a significant overlap between the representations of ground-truth
              captions and hallucinated captions. The model struggles to distinguish fact from fiction.<br>
              <strong>After SANTA:</strong> The features are clearly separated. The model has learned a distinct
              boundary between faithful descriptions and hallucinations.
            </p>
          </div>
        </div>
      </div>
      <!--/ Analysis. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Conclusion. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Conclusion</h2>
          <div class="content has-text-justified">
            <p>
              We presented <strong>SANTA</strong>, a novel framework to mitigate object and action hallucinations in
              Multimodal LLMs. By leveraging <strong>Self-Augmented Contrastive Alignment</strong>, we enable models to:
            </p>
            <ul>
              <li>Identify and suppress their own tendency to hallucinate via self-generated hard negatives.</li>
              <li>Ground language tokens in fine-grained visual evidence through tracklet-phrase alignment.</li>
              <li>Achieve state-of-the-art performance in factual video captioning and hallucination benchmarks.</li>
            </ul>
            <p>
              SANTA offers a robust solution for deploying MLLMs in high-stakes real-world applications where
              faithfulness is paramount.
            </p>
          </div>
        </div>
      </div>
      <!--/ Conclusion. -->
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{SANTA2026,
  title={Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment},
  author={Anonymous Authors},
  journal={WACV Algorithms Track},
  year={2026}
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://drive.google.com/file/d/130I3DZtX5LPSsPdQVLQpvBmxyLJo_4Ny/view?usp=sharing">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/kaipochang" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              Template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>